---
title: "PSTAT 131 Homework Three: Classification"
author: "Carly Greutert"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, include=FALSE}
library(tidymodels)
library(ggplot2)
library(discrim)
library(corrr)
library(klaR) # for naive bayes
library(caret)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(corrplot)
library(ggthemes)
library(cli)
library(recipes)
library(pROC)
library(yardstick)
library(MASS)
library(poissonreg)
library(naivebayes)
tidymodels_prefer()
```

```{r message=FALSE, include=FALSE}
titanic <- read_csv('C:\\Program Files\\Git\\tmp\\131-hw3\\titanic.csv')
names <- c('pclass', 'survived')
titanic[,names] <- lapply(titanic[,names] , factor)
```

Note I used https://www.listendata.com/2015/05/converting-multiple-numeric-variables.html#:~:text=In%20R%2C%20you%20can%20convert%20multiple%20numeric%20variables,functions.%20They%20perform%20multiple%20iterations%20%28loops%29%20in%20R to learn about using the lapply function.

1.
```{r}
set.seed(777)
titanic_split <- initial_split(titanic, prop = 0.80, strata = 'survived')
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```
Note that since the training data is 80% of the data set observations and titanic_train has 891\*0.8~712 observations and titanic_test is 891\*0.2~179 observations. 
```{r}
sum(is.na(titanic_train$passenger_id))
sum(is.na(titanic_train$survived))
sum(is.na(titanic_train$pclass))
sum(is.na(titanic_train$name))
sum(is.na(titanic_train$sex))
sum(is.na(titanic_train$age))
sum(is.na(titanic_train$sib_sp))
sum(is.na(titanic_train$parch))
sum(is.na(titanic_train$ticket))
sum(is.na(titanic_train$fare))
sum(is.na(titanic_train$cabin))
sum(is.na(titanic_train$embarked))
```
Note that there are a significant number of observations missing for age, but a significant amount more for cabin. All other column observations either have 0 or 1 missing observations. Also, it is clear that we should use stratified sampling for this data set because of the diverse set of influences on whether or not a passenger survived based on different variables (i.e. sex, age, pclass, etc). Thus, our sample should include a diverse mix of these possible influences.                                                                                     
2. 
```{r}
titanic_train %>% ggplot(aes(x = factor(survived)))+ stat_count(geom = "bar")
```

From the distribution of our outcome variable, survived, there were significantly more passengers who did not survive, as opposed to those who did.                                   
3.
```{r}
titanic_cor <- titanic_train %>%
  select(-survived) %>%
  select(-pclass)%>%
  select(-sex) %>%
  select(-embarked) %>%
  select(-cabin) %>%
  select(-ticket) %>%
  select(-name) %>%
  correlate(use = "pairwise.complete.obs",
  method = "pearson")
rplot(titanic_cor)
```

I notice that the variables parch and sib_sp are positively correlated and parch and age, as well as age and sib_sp are negatively correlated (i.e in the increasing and decreasing direction, respectively).                                                                       
4.
```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train)
titanic_recipe <- titanic_recipe %>%
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors())%>%
  step_interact(~sex:fare) %>%
  step_interact(~age:fare)
titanic_recipe
```

5.
```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)
log_fit <- fit(log_wkflow, titanic_train)
```

6.
```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

lda_fit <- fit(lda_wkflow, titanic_train)
```
7.
```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)

qda_fit <- fit(qda_wkflow, titanic_train)
```
8.
```{r}
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("naivebayes") %>% 
  set_args(usekernel = FALSE) 

nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(titanic_recipe)

nb_fit <- fit(nb_wkflow, titanic_train)
```
9.
```{r}
predict(log_fit, new_data = titanic_train, type = "prob")
predict(lda_fit, new_data = titanic_train, type = "prob")
predict(qda_fit, new_data = titanic_train, type = "prob")
predict(nb_fit, new_data = titanic_train, type = "prob")
log_reg_acc <- augment(log_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
lda_acc <- augment(lda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
qda_acc <- augment(qda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
nb_acc <- augment(nb_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)

accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```

The logistic model received the highest score of accuracy (0.810) compared to the rest of the models.                                                                                       
10.
```{r}
pred <- predict(log_fit, new_data = titanic_test, type = "prob")
pred
multi_metric <- metric_set(accuracy, yardstick::sensitivity, yardstick::specificity)
augment(log_fit, new_data = titanic_test) %>%
  multi_metric(truth = survived, estimate = .pred_class)
augment(log_fit, new_data = titanic_test) %>%
  conf_mat(truth = survived, estimate = .pred_class) 
augment(log_fit, new_data = titanic_test) %>%
  roc_curve(survived, .pred_No) %>%
  autoplot()
auc(titanic_test$survived, pred$.pred_No)
```

The AUC of the ROC is 0.8663. The accuracy of our logistic model on the testing data ended up being roughly 0.838, which tells us our model fits the data decently well. It is also a higher accuracy estimate than the training data (0.810), but not by much. A likely explanation for this is that there are fewer observations in our testing data, so it is likely our data set does not include as many outliers, which may influence the accuracy. 